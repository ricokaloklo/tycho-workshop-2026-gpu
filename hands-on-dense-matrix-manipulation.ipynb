{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c077da7f",
   "metadata": {},
   "source": [
    "# Hands-on demos/exercises 1: Dense matrix manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4704dea6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We compute\n",
    "\n",
    "$$\n",
    "y = W x + b\n",
    "$$\n",
    "\n",
    "with:\n",
    "- $ W \\in \\mathbb{R}^{M \\times N} $\n",
    "- $ x \\in \\mathbb{R}^{N} $\n",
    "- $ b \\in \\mathbb{R}^{M} $\n",
    "\n",
    "where $M, N \\gg 1$, using five different implementations :\n",
    "\n",
    "1. Pure Python (no NumPy)\n",
    "2. NumPy\n",
    "3. CuPy\n",
    "4. JAX\n",
    "5. PyTorch [optional]\n",
    "\n",
    "Credit: everything is generated using ChatGPT 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b82ef6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def format_time(seconds, iters):\n",
    "    avg = seconds / iters\n",
    "    return f\"total = {seconds:.4f}s, per-iter = {avg*1e3:.3f} ms\"\n",
    "\n",
    "\n",
    "def benchmark(fn, setup_fn, iters=10, warmup=2, sync_fn=None, name=\"\"):\n",
    "    \"\"\"\n",
    "    Generic benchmark helper.\n",
    "\n",
    "    setup_fn: () -> tuple of arguments for fn\n",
    "    fn: (*args) -> result\n",
    "    sync_fn: optional function called as sync_fn(result) to synchronize GPU\n",
    "    \"\"\"\n",
    "    args = setup_fn()\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        y = fn(*args)\n",
    "        if sync_fn is not None:\n",
    "            sync_fn(y)\n",
    "\n",
    "    # Timed runs\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        y = fn(*args)\n",
    "        if sync_fn is not None:\n",
    "            sync_fn(y)\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    print(f\"{name:>12}: {format_time(end - start, iters)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2096b1",
   "metadata": {},
   "source": [
    "## Implementation using pure python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81074c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def py_dense(W, x, b):\n",
    "    \"\"\"Compute y = W x + b using pure Python lists.\"\"\"\n",
    "    M = len(W)\n",
    "    y = [0.0] * M\n",
    "    for i in range(M):\n",
    "        s = 0.0\n",
    "        row = W[i]\n",
    "        # manual dot product\n",
    "        for j in range(len(x)):\n",
    "            s += row[j] * x[j]\n",
    "        y[i] = s + b[i]\n",
    "    return y\n",
    "\n",
    "\n",
    "def make_python_data(M, N, seed=0):\n",
    "    \"\"\"Random Python lists with shapes:\n",
    "    - W: M x N\n",
    "    - x: N\n",
    "    - b: M\n",
    "    \"\"\"\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    W = [[random.random() for _ in range(N)] for _ in range(M)]\n",
    "    x = [random.random() for _ in range(N)]\n",
    "    b = [random.random() for _ in range(M)]\n",
    "    return (W, x, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c690ae",
   "metadata": {},
   "source": [
    "## Implementation using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca433c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "\n",
    "    def np_dense(W, x, b):\n",
    "        # W: (M, N), x: (N,), b: (M,)\n",
    "        return W @ x + b\n",
    "\n",
    "    def make_numpy_data(M, N, seed=0):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        W = rng.random((M, N), dtype=np.float32)\n",
    "        x = rng.random((N,), dtype=np.float32)\n",
    "        b = rng.random((M,), dtype=np.float32)\n",
    "        return (W, x, b)\n",
    "\n",
    "    HAS_NUMPY = True\n",
    "except ImportError:\n",
    "    HAS_NUMPY = False\n",
    "    print(\"NumPy not installed; NumPy benchmark will be skipped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd174e",
   "metadata": {},
   "source": [
    "## Implementation using cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f08b7535",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import cupy as cp\n",
    "\n",
    "    def cp_dense(W, x, b):\n",
    "        return W @ x + b\n",
    "\n",
    "    def make_cupy_data(M, N, seed=0):\n",
    "        cp.random.seed(seed)\n",
    "        W = cp.random.random((M, N), dtype=cp.float32)\n",
    "        x = cp.random.random((N,), dtype=cp.float32)\n",
    "        b = cp.random.random((M,), dtype=cp.float32)\n",
    "        return (W, x, b)\n",
    "\n",
    "    def cupy_sync(y):\n",
    "        # Make sure all kernels are done\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "\n",
    "    # Quick device check\n",
    "    _ = cp.cuda.Device(0)\n",
    "    HAS_CUPY = True\n",
    "except ImportError:\n",
    "    HAS_CUPY = False\n",
    "    print(\"CuPy not installed; CuPy benchmark will be skipped.\")\n",
    "except Exception as e:\n",
    "    HAS_CUPY = False\n",
    "    print(f\"CuPy found but not usable ({e}); CuPy benchmark will be skipped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952ae45d",
   "metadata": {},
   "source": [
    "## Implementation using jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f9e5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "\n",
    "    @jax.jit\n",
    "    def jax_dense(W, x, b):\n",
    "        return W @ x + b\n",
    "\n",
    "    def make_jax_data(M, N, seed=0):\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "        key_W, key_x, key_b = jax.random.split(key, 3)\n",
    "        W = jax.random.uniform(key_W, (M, N), dtype=jnp.float32)\n",
    "        x = jax.random.uniform(key_x, (N,), dtype=jnp.float32)\n",
    "        b = jax.random.uniform(key_b, (M,), dtype=jnp.float32)\n",
    "        return (W, x, b)\n",
    "\n",
    "    def jax_sync(y):\n",
    "        # Ensure computation completes (important on accelerators)\n",
    "        y.block_until_ready()\n",
    "\n",
    "    HAS_JAX = True\n",
    "except ImportError:\n",
    "    HAS_JAX = False\n",
    "    print(\"JAX not installed; JAX benchmark will be skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b6f5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "\n",
    "    def torch_dense(W, x, b):\n",
    "        return W @ x + b\n",
    "\n",
    "    def make_torch_data(M, N, seed=0, device=\"cpu\"):\n",
    "        torch.manual_seed(seed)\n",
    "        W = torch.rand((M, N), dtype=torch.float32, device=device)\n",
    "        x = torch.rand((N,), dtype=torch.float32, device=device)\n",
    "        b = torch.rand((M,), dtype=torch.float32, device=device)\n",
    "        return (W, x, b)\n",
    "\n",
    "    def torch_sync(y):\n",
    "        if y.is_cuda:\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    HAS_TORCH = True\n",
    "except ImportError:\n",
    "    HAS_TORCH = False\n",
    "    print(\"PyTorch not installed; PyTorch benchmark will be skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e3e4f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "\n",
    "    def torch_dense(W, x, b):\n",
    "        return W @ x + b\n",
    "\n",
    "    def make_torch_data(M, N, seed=0, device=\"cpu\"):\n",
    "        torch.manual_seed(seed)\n",
    "        W = torch.rand((M, N), dtype=torch.float32, device=device)\n",
    "        x = torch.rand((N,), dtype=torch.float32, device=device)\n",
    "        b = torch.rand((M,), dtype=torch.float32, device=device)\n",
    "        return (W, x, b)\n",
    "\n",
    "    def torch_sync(y):\n",
    "        if y.is_cuda:\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    HAS_TORCH = True\n",
    "except ImportError:\n",
    "    HAS_TORCH = False\n",
    "    print(\"PyTorch not installed; PyTorch benchmark will be skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2f07043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking y = W x + b with:\n",
      "  W: (16384, 16384)\n",
      "  x: (16384,)\n",
      "  b: (16384,)\n",
      "iters = 10, warmup = 2\n",
      "------------------------------------------------------------\n",
      " pure_python: total = 193.5661s, per-iter = 19356.609 ms\n",
      "       numpy: total = 0.4812s, per-iter = 48.124 ms\n",
      "        cupy: total = 0.0080s, per-iter = 0.803 ms\n",
      "         jax: total = 0.0083s, per-iter = 0.832 ms\n",
      "  torch_cuda: total = 0.0078s, per-iter = 0.784 ms\n"
     ]
    }
   ],
   "source": [
    "# ---- Configuration ----\n",
    "M = 16_384        # rows of W and length of b\n",
    "N = 16_384        # cols of W and length of x\n",
    "iters = 10\n",
    "warmup = 2\n",
    "\n",
    "# Toggle this to \"cuda\" if you want to test GPU (and have it available)\n",
    "torch_device = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "print(f\"Benchmarking y = W x + b with:\")\n",
    "print(f\"  W: ({M}, {N})\")\n",
    "print(f\"  x: ({N},)\")\n",
    "print(f\"  b: ({M},)\")\n",
    "print(f\"iters = {iters}, warmup = {warmup}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 1) Pure Python\n",
    "try:\n",
    "    def setup_py():\n",
    "        return make_python_data(M, N)\n",
    "    benchmark(py_dense, setup_py, iters=iters, warmup=warmup,\n",
    "              sync_fn=None, name=\"pure_python\")\n",
    "except MemoryError:\n",
    "    print(f\"{'pure_python':>12}: skipped (MemoryError - too large M,N)\")\n",
    "\n",
    "# 2) NumPy\n",
    "if HAS_NUMPY:\n",
    "    def setup_np():\n",
    "        return make_numpy_data(M, N)\n",
    "    benchmark(np_dense, setup_np, iters=iters, warmup=warmup,\n",
    "              sync_fn=None, name=\"numpy\")\n",
    "\n",
    "# 3) CuPy\n",
    "if HAS_CUPY:\n",
    "    def setup_cp():\n",
    "        return make_cupy_data(M, N)\n",
    "    benchmark(cp_dense, setup_cp, iters=iters, warmup=warmup,\n",
    "              sync_fn=cupy_sync, name=\"cupy\")\n",
    "\n",
    "# 4) JAX\n",
    "if HAS_JAX:\n",
    "    def setup_jax():\n",
    "        return make_jax_data(M, N)\n",
    "    benchmark(jax_dense, setup_jax, iters=iters, warmup=warmup,\n",
    "              sync_fn=jax_sync, name=\"jax\")\n",
    "\n",
    "# 5) PyTorch\n",
    "if HAS_TORCH:\n",
    "    if torch_device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(f\"{'torch_cuda':>12}: skipped (CUDA not available)\")\n",
    "    else:\n",
    "        def setup_torch():\n",
    "            return make_torch_data(M, N, device=torch_device)\n",
    "        name = \"torch_cuda\" if torch_device == \"cuda\" else \"torch_cpu\"\n",
    "        benchmark(torch_dense, setup_torch, iters=iters, warmup=warmup,\n",
    "                  sync_fn=torch_sync, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65f668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tycho_workshop_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
